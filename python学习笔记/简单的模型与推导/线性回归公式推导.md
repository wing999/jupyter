# 线性回归公式推导
公式为：
$$ h_{\theta}(x) = \theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2} $$

> 1. 以上公式可认为 $x_{0}=1$ ,
> 2. $\theta_{i}$为列向量，$x_{i}$也为列向量，$\sum_{i=0}^{n}\theta_{i}x_{i}$ 则可转化为 $\theta^{T}x$
> 3. 通过转化后可得到如下公式
> 
$$h_{\theta}(x) = \sum_{i=0}^{n}\theta_{i}x_{i} = \theta^{T}x$$

---

> 因为预测值  $\hat{y}$ 与实际值$y^{i}$之间肯定存在误差$\varepsilon^{i}$
> 
$$y^{(i)} = \hat{y}+\varepsilon^{(i)}$$
$$ \varepsilon^{(i)}=y^{(i)}-\hat{y}$$

---

> 1. 误差$\varepsilon^{i}$是**独立(每条样本数据之间是无联系的)**并且具有相同的分布，通常认为服从均值$\mu$为0，方差为$\theta^{2}$ 的**高斯分布(正太分布)**
> 2. 正态分布计算的结果其实为概率密度函数
> 3. 通过转化得到以下函数
> 
$$ p(\varepsilon^{(i)}) = \frac{1}{\sqrt{2\pi\sigma}}exp(-\frac{(\varepsilon^{(i)}-\mu)^{2}}{2\sigma^{2}})=\frac{1}{\sqrt{2\pi\sigma}}exp(-\frac{(\varepsilon^{(i)})^{2}}{2\sigma^{2}})$$

------

> 1. $p(y^{(i)}|x^{(i)};\theta)$ 的含义：什么样的$\theta$跟$x^{(i)}$组合后，能够越接近真实值$y^{i}$的概率最大。
$$ p(y^{(i)}|x^{(i)};\theta) = \frac{1}{\sqrt{2\pi\sigma}}exp(-\frac{(y^{(i)}-\theta^{T}x^{(i)})^{2}}{2\sigma^{2}})$$

---

> 1. 最大似然估计$L(\theta)$
> 2. 因为$\theta$ 要与每个$x$相乘，所以需要累乘$p(y^{(i)}|x^{(i)};\theta)$ 使得结果概率$L(\theta)$最大。
> $$ L(\theta)=\prod_{i=1}^{m}p(y^{(i)}|x^{(i)};\theta) = \prod_{i=1}^{m}\frac{1}{\sqrt{2\pi\sigma}}exp(-\frac{(y^{(i)}-\theta^{T}x^{(i)})^{2}}{2\sigma^{2}})$$

---

> 1. 对数自然函数
> 2. 求累乘的最大极值问题比较困难，需要进行转换
> 3. 这里运用到函数$logAB = logA+logB$
> $$l(\theta)  = \log{L(\theta)} \tag{1}$$
> $$=\log{\prod_{i=1}^{m}\frac{1}{\sqrt{2\pi\sigma}}exp(-\frac{(y^{(i)}-\theta^{T}x^{(i)})^{2}}{2\sigma^{2}})} \tag{2}$$
> $$=\sum_{x=1}^{m}\log{\frac{1}{\sqrt{2\pi\sigma}}exp(-\frac{(y^{(i)}-\theta^{T}x^{(i)})^{2}}{2\sigma^{2}})}\tag{3}$$
> $$=m\log{\frac{1}{\sqrt{2\pi\sigma}}}-\frac{1}{\sigma^{2}}\frac{1}{2}\sum_{i=1}^{m}(y^{(i)}-\theta^{T}x^{(i)})^{2} \tag{4}$$
> 4. 因为要使得最后的值最大。$m\log{\frac{1}{\sqrt{2\pi\sigma}}}$为常数，所以要使得$\frac{1}{\sigma^{2}}\frac{1}{2}\sum_{i=1}^{m}(y^{(i)}-\theta^{T}x^{(i)})^{2}$最小即可。
> $$J(\theta)=\frac{1}{2}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^{2}$$

---

